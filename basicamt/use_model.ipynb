{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append(\"../model\")\n",
    "\n",
    "model_path = \"best_basicamt_model.pth\"\n",
    "model = torch.load(model_path, map_location=torch.device('cpu'), weights_only=False)\n",
    "s_per_frame = 256 / 22050\n",
    "\n",
    "# 二值化\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.midiarray import output2midi, midi2numpy\n",
    "\n",
    "def create_note(audio_name, onset, note, s_per_frame, onset_threshold=0.14, note_threshold=0.3):\n",
    "    mid = output2midi(onset, note, s_per_frame, note_threshold, onset_threshold)\n",
    "    mid.save(f\"../test/{audio_name}.mid\")\n",
    "    print(f\"saved midi to '../test/{audio_name}.mid'\")\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.title('Note')\n",
    "    plt.imshow(midi2numpy(mid, s_per_frame), aspect='auto', origin='lower', cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "    return mid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对于44100Hz的输入，可以用smallCQT自带的降采样函数\n",
    "import torchaudio\n",
    "from utils.midiarray import midi2numpy\n",
    "from utils.wavtool import waveInfo\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "input_wav_path = \"../data/inferMusic/孤独な巡礼simple.wav\"\n",
    "\n",
    "audio_name = os.path.basename(input_wav_path).split('.')[0]\n",
    "\n",
    "try:    # 尝试加载同一目录下同名midi文件\n",
    "    midi_path = os.path.join(os.path.dirname(input_wav_path), audio_name + '.mid')\n",
    "    midi_array = midi2numpy(midi_path, s_per_frame)\n",
    "except:\n",
    "    midi_array = None\n",
    "\n",
    "waveform, sample_rate = torchaudio.load(input_wav_path, normalize=True)\n",
    "waveform = waveform.unsqueeze(0)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # 降采样到22050Hz\n",
    "    if sample_rate == 44100:\n",
    "        waveform = model.cqt.down2sample(waveform)\n",
    "        print(\"downsampled to 22050Hz\")\n",
    "    elif sample_rate != 22050:\n",
    "        # 降采样\n",
    "        wav_path = f\"../test/{audio_name}_22050.wav\"\n",
    "        os.system(f'ffmpeg -i \"{input_wav_path}\" -vn -ar 22050 -y \"{wav_path}\"')\n",
    "        print(\"saved downsampled wav to\", wav_path)\n",
    "        waveInfo(wav_path)\n",
    "        waveform, sample_rate = torchaudio.load(wav_path, normalize=True)\n",
    "        waveform = waveform.unsqueeze(0)\n",
    "\n",
    "    onset, note = model(waveform)\n",
    "    onset = onset.cpu().numpy()[0]\n",
    "    note = note.cpu().numpy()[0]\n",
    "    # 归一化\n",
    "    note = note / np.max(note)\n",
    "    onset = onset / np.max(onset)\n",
    "\n",
    "if midi_array is None:\n",
    "    plot_num = 2\n",
    "    plt.figure(figsize=(12, plot_num * 5))\n",
    "else:\n",
    "    plot_num = 3\n",
    "    plt.figure(figsize=(12, plot_num * 5))\n",
    "    plt.subplot(plot_num, 1, 1)\n",
    "    plt.title('Midi')\n",
    "    plt.imshow(midi_array, aspect='auto', origin='lower', cmap='gray')\n",
    "\n",
    "plt.subplot(plot_num, 1, plot_num - 1)\n",
    "plt.title('Note')\n",
    "plt.imshow(note, aspect='auto', origin='lower', cmap='gray')\n",
    "\n",
    "plt.subplot(plot_num, 1, plot_num)\n",
    "plt.title('Onset')\n",
    "plt.imshow(onset, aspect='auto', origin='lower', cmap='gray')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 二值化\n",
    "mid = create_note(\"origin_\"+audio_name, onset, note, s_per_frame, note_threshold=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"note.npy\", note)\n",
    "np.save(\"onset.npy\", onset)\n",
    "plt.imsave(\"note.png\", note, origin='lower', cmap='gray')\n",
    "plt.imsave(\"onset.png\", onset, origin='lower', cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导出为接收44100Hz输入的ONNX模型\n",
    "为了能在网页上使用，需要导出为onnx模型，接收44100Hz单通道音频输入。\n",
    "需要额外安装以下内容：\n",
    "```bash\n",
    "pip install onnx\n",
    "pip install onnxruntime\n",
    "pip install onnxscript\n",
    "```\n",
    "注意onnxruntime有GPU版本，指令不一样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from basicamt import BasicAMT_44100\n",
    "\n",
    "model_44100 = BasicAMT_44100(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试新模型\n",
    "import torchaudio\n",
    "from utils.midiarray import midi2numpy\n",
    "from utils.wavtool import waveInfo\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "input_wav_path = \"../data/inferMusic/piano_short.wav\"\n",
    "\n",
    "audio_name = os.path.basename(input_wav_path).split('.')[0]\n",
    "\n",
    "try:    # 尝试加载同一目录下同名midi文件\n",
    "    midi_path = os.path.join(os.path.dirname(input_wav_path), audio_name + '.mid')\n",
    "    midi_array = midi2numpy(midi_path, s_per_frame)\n",
    "except:\n",
    "    midi_array = None\n",
    "\n",
    "waveform, sample_rate = torchaudio.load(input_wav_path, normalize=True)\n",
    "waveform = waveform.mean(-2, keepdim=True)\n",
    "waveform = waveform.unsqueeze(0)\n",
    "\n",
    "model_44100.eval()\n",
    "with torch.no_grad():\n",
    "    onset, note = model_44100(waveform)\n",
    "    onset = onset.cpu().numpy()[0]\n",
    "    note = note.cpu().numpy()[0]\n",
    "    # 归一化（包含在模型内部）\n",
    "    # note = note / np.max(note)\n",
    "    # onset = onset / np.max(onset)\n",
    "\n",
    "if midi_array is None:\n",
    "    plot_num = 2\n",
    "    plt.figure(figsize=(12, plot_num * 5))\n",
    "else:\n",
    "    plot_num = 3\n",
    "    plt.figure(figsize=(12, plot_num * 5))\n",
    "    plt.subplot(plot_num, 1, 1)\n",
    "    plt.title('Midi')\n",
    "    plt.imshow(midi_array, aspect='auto', origin='lower', cmap='gray')\n",
    "\n",
    "plt.subplot(plot_num, 1, plot_num - 1)\n",
    "plt.title('Note')\n",
    "plt.imshow(note, aspect='auto', origin='lower', cmap='gray')\n",
    "\n",
    "plt.subplot(plot_num, 1, plot_num)\n",
    "plt.title('Onset')\n",
    "plt.imshow(onset, aspect='auto', origin='lower', cmap='gray')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "mid = create_note(\"new_\"+audio_name, onset, note, s_per_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导出为ONNX\n",
    "model_44100.eval()\n",
    "\n",
    "input_audio = torch.randn((1, 1, 22050), dtype=torch.float32)    # (fixed, fixed, dynamic)\n",
    "\n",
    "with torch.no_grad():\n",
    "    torch.onnx.export(\n",
    "        model_44100,\n",
    "        (input_audio,),\n",
    "        'basicamt_44100.onnx',\n",
    "        input_names = [\"audio\"],\n",
    "        output_names = [\"onset\",\"frame\"],\n",
    "        dynamic_shapes = {\n",
    "            'x': {2:'time'},    # same as model.forward input\n",
    "            # auto infer output shapes\n",
    "        },\n",
    "        dynamo=True,\n",
    "        verbose=False,\n",
    "        verify=True,\n",
    "        external_data=False,\n",
    "        autograd_inlining=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用onnxruntime推理\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "import torchaudio\n",
    "import os\n",
    "\n",
    "ort_session = ort.InferenceSession(\"basicamt_44100.onnx\") # 创建一个推理session\n",
    "\n",
    "# input_wav_path = \"D:/Music/加藤達也-Always in my heart.mp3\"\n",
    "# input_wav_path = \"D:/Music/两个人总是-秋山裕和.mp3\"\n",
    "input_wav_path = \"../data/inferMusic/piano_short.wav\"\n",
    "\n",
    "waveform, sample_rate = torchaudio.load(input_wav_path, normalize=True)\n",
    "audio_name = os.path.basename(input_wav_path).split('.')[0]\n",
    "\n",
    "waveform = waveform.unsqueeze(0).cpu()\n",
    "if waveform.shape[1] > 1:\n",
    "    waveform = waveform.mean(1, keepdim=True)\n",
    "waveform = waveform.numpy().astype(np.float32)\n",
    "\n",
    "outputs = ort_session.run(None, {'audio': waveform})\n",
    "onset = outputs[0][0]\n",
    "note = outputs[1][0]\n",
    "# onset = onset / np.max(onset)\n",
    "# note = note / np.max(note)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.title('Note')\n",
    "plt.imshow(note, aspect='auto', origin='lower', cmap='gray')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.title('Onset')\n",
    "plt.imshow(onset, aspect='auto', origin='lower', cmap='gray')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# 二值化\n",
    "mid = create_note(\"onnx_\"+audio_name, onset, note, s_per_frame)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "timbreamt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
