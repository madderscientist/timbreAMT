1:	train_loss: 487.4857	val_loss: 675.6412	413844.0734	243.8865	520262.5547	338.0401
2:	train_loss: 464.7739	val_loss: 691.4109	390700.0968	232.5253	512898.0000	345.9386
3:	train_loss: 450.4052	val_loss: 670.1818	375439.1314	225.3378	510667.9818	335.3109
4:	train_loss: 442.2694	val_loss: 671.5356	364546.3447	221.2689	503372.3967	335.9919
5:	train_loss: 436.2904	val_loss: 642.2474	355400.3995	218.2792	496134.5421	321.3317
6:	train_loss: 427.7362	val_loss: 650.8881	346629.8927	214.0001	498194.8151	325.6568
7:	train_loss: 422.9983	val_loss: 748.7308	339752.8774	211.6309	507214.4479	374.6420
8:	train_loss: 419.5857	val_loss: 634.8931	333455.7226	209.9249	501970.4132	317.6474
9:	train_loss: 413.6605	val_loss: 629.9788	327697.9004	206.9609	500884.1389	315.1876
10:	train_loss: 411.1022	val_loss: 637.0776	322706.0421	205.6821	510393.9800	318.7377
11:	train_loss: 406.6548	val_loss: 665.1939	317851.4665	203.4576	508077.3021	332.8148
12:	train_loss: 404.5283	val_loss: 664.7721	313535.7678	202.3947	505770.4236	332.6046
13:	train_loss: 400.9973	val_loss: 632.6251	309572.8115	200.6286	504828.0885	316.5109
14:	train_loss: 397.4929	val_loss: 632.5065	305511.5620	198.8758	500684.5104	316.4532
15:	train_loss: 396.6516	val_loss: 697.8671	303018.8698	198.4557	514674.4861	349.1703
16:	train_loss: 393.0395	val_loss: 650.9046	299348.1528	196.6488	502614.1758	325.6632
17:	train_loss: 391.7150	val_loss: 653.9099	296833.0117	195.9868	503621.1033	327.1674
18:	train_loss: 387.6892	val_loss: 662.0421	293647.2794	193.9726	502880.1068	331.2391
19:	train_loss: 386.8099	val_loss: 760.0141	290920.5868	193.5336	510109.1615	380.2904
20:	train_loss: 385.7589	val_loss: 628.8173	288426.2707	193.0085	506038.9549	314.6041
21:	train_loss: 382.7611	val_loss: 653.3563	285861.4923	191.5088	512937.3229	326.8863
22:	train_loss: 382.0116	val_loss: 671.5762	283422.3776	191.1346	505433.1562	336.0113
23:	train_loss: 380.7517	val_loss: 636.9572	282190.4590	190.5044	516713.7257	318.6750
24:	train_loss: 379.1307	val_loss: 695.9465	279045.8433	189.6942	520005.1806	348.2063
25:	train_loss: 376.4611	val_loss: 687.7451	277247.8926	188.3584	530084.1606	344.0957
26:	train_loss: 377.0494	val_loss: 654.3958	275395.8317	188.6539	515969.5269	327.4055
27:	train_loss: 373.8762	val_loss: 651.1796	273639.4857	187.0659	514675.2769	325.7959
28:	train_loss: 372.8217	val_loss: 653.0055	272062.9944	186.5386	514707.1068	326.7100
29:	train_loss: 371.4539	val_loss: 677.6582	270163.3206	185.8547	523107.7188	339.0487
30:	train_loss: 371.1147	val_loss: 662.1409	269694.3014	185.6851	512995.2118	331.2842
31:	train_loss: 370.3444	val_loss: 668.7161	268091.4979	185.3002	519409.7040	334.5734
32:	train_loss: 368.5099	val_loss: 668.4174	265920.1142	184.3827	514406.8993	334.4260
33:	train_loss: 367.4108	val_loss: 640.6782	264772.0320	183.8330	511244.4913	320.5399
34:	train_loss: 366.4269	val_loss: 656.4348	263603.4863	183.3409	506116.7578	328.4304
35:	train_loss: 364.8362	val_loss: 655.9258	261588.2568	182.5454	509433.6128	328.1742
36:	train_loss: 364.0232	val_loss: 638.1512	260496.1092	182.1389	521070.0226	319.2711
37:	train_loss: 363.6480	val_loss: 662.4907	258948.2535	181.9518	516591.0885	331.4579
