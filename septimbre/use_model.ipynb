{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append(\"../model\")\n",
    "model = torch.load(\"sepamt_model.pth\", map_location=torch.device('cpu'), weights_only=False)\n",
    "s_per_frame = 256 / 22050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "from utils.midiarray import midi2numpy\n",
    "\n",
    "def draw_midi_with_channel(ax, midiarr, colors=['Reds', 'Blues'], labels=['Piano', 'Violin']):\n",
    "    \"\"\"\n",
    "    用不同颜色绘制midiarray\n",
    "    ax: 通常是plt的subplot的一张图\n",
    "    midiarr: 3D numpy array, shape = (n_channel, n_time, n_pitch) 或者midi路径\n",
    "    colors: 颜色列表，名字需要和plt.cm中的颜色一致\n",
    "    \"\"\"\n",
    "\n",
    "    # 从文件打开midi\n",
    "    if isinstance(midiarr, str):\n",
    "        midiarr = midi2numpy(midiarr, s_per_frame, track_separate=True)\n",
    "        if len(midiarr.shape) != 3:\n",
    "            raise ValueError(\"midiarr must be a 3D numpy array\")\n",
    "\n",
    "    Colors = [plt.get_cmap(color) if isinstance(color, str) else color for color in colors]\n",
    "    # 白色背景\n",
    "    background = np.ones_like(midiarr[0])\n",
    "    ax.imshow(background, cmap='gray_r', aspect='auto', origin='lower')\n",
    "\n",
    "    for i, ch in enumerate(midiarr):\n",
    "        # 预处理红色通道\n",
    "        red_data = np.zeros_like(ch, dtype=float)\n",
    "        alpha_red = np.zeros_like(ch, dtype=float)\n",
    "        # 设置不同透明度\n",
    "        red_data[ch > 0] = 1.0  # 红色值\n",
    "        alpha_red[ch == 1] = 0.6  # 半透明\n",
    "        alpha_red[ch == 2] = 1.0  # 不透明\n",
    "        # 绘制红色（使用 RGBA 数组）\n",
    "        red_rgba = Colors[i](red_data)  # 获取 RGBA 颜色\n",
    "        red_rgba[..., 3] = alpha_red  # 修改透明度通道\n",
    "        ax.imshow(red_rgba, aspect='auto', origin='lower')\n",
    "\n",
    "    if len(labels) != len(colors):\n",
    "        return\n",
    "    legend_elements = [Patch(facecolor=Colors[i](0.8), alpha=0.6, label=labels[i]) for i in range(len(labels))]\n",
    "    ax.legend(handles=legend_elements, loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基本使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取音频\n",
    "import torchaudio\n",
    "from utils.wavtool import waveInfo\n",
    "from utils.midiarray import midi2numpy\n",
    "import os\n",
    "\n",
    "# test_wave_path = \"../data/inferMusic/short mix.wav\"\n",
    "# test_wave_path = \"../data/inferMusic/three_mix.wav\"\n",
    "test_wave_path = \"../data/inferMusic/flute_violin_piano.wav\"\n",
    "# test_wave_path = \"../data/inferMusic/flute_guitar_Humoresque.wav\"\n",
    "# test_wave_path = \"../data/inferMusic/flute_violin.wav\"\n",
    "# test_wave_path = \"../data/inferMusic/孤独な巡礼simple.wav\"\n",
    "\n",
    "# 尝试获取midi\n",
    "test_midi = os.path.splitext(test_wave_path)[0] + \".mid\"\n",
    "if os.path.exists(test_midi):\n",
    "    test_midi = midi2numpy(test_midi, s_per_frame, track_separate=True)\n",
    "else:\n",
    "    test_midi = None\n",
    "\n",
    "info = waveInfo(test_wave_path)\n",
    "\n",
    "waveform, sample_rate = torchaudio.load(test_wave_path, normalize=True)\n",
    "waveform = waveform.unsqueeze(0)\n",
    "if info[\"sample_rate\"] > 44000:\n",
    "    waveform = model.cqt.down2sample(waveform)\n",
    "    print(f\"$ downsampled to {info[\"sample_rate\"]//2}Hz\")\n",
    "print(waveform.shape)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    onset, mask, emb = model(waveform)\n",
    "    emb = emb / torch.sqrt(emb.pow(2).sum(dim=1, keepdim=True) + 1e-8)\n",
    "    emb = emb.cpu().numpy()[0]      # (18, 84, frame)\n",
    "    mask = mask.cpu().numpy()[0]    # (84, frame)\n",
    "    onset = onset.cpu().numpy()[0]\n",
    "\n",
    "if test_midi is not None:\n",
    "    emb = emb[:, :, :test_midi.shape[2]]  # 截取到和midi一样长\n",
    "    mask = mask[:, :test_midi.shape[2]]    # 截取到和midi一样长\n",
    "    onset = onset[:, :test_midi.shape[2]]  # 截取到和midi一样长"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用聚类\n",
    "需要明确类别数目"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N = 3 if test_midi is None else len(test_midi)  # 聚类数目\n",
    "\n",
    "# mask大于阈值的数目记为n\n",
    "positions = np.where(mask > 0.5)\n",
    "emb_extracted = emb[:, positions[0], positions[1]].T        # (n, 18)\n",
    "\n",
    "# 计算余弦相似度矩阵\n",
    "similarity_matrix = cosine_similarity(emb_extracted)\n",
    "\n",
    "# 进行谱聚类\n",
    "spectral = SpectralClustering(n_clusters=N, affinity='precomputed', assign_labels=\"cluster_qr\")\n",
    "labels = spectral.fit_predict(np.exp(similarity_matrix))\n",
    "\n",
    "pre_figures = 2 + (0 if test_midi is None else 1)\n",
    "sub_figures = N + pre_figures\n",
    "\n",
    "plt.figure(figsize=(10, 5*sub_figures))\n",
    "\n",
    "if test_midi is not None:\n",
    "    plt.subplot(sub_figures, 1, 1)\n",
    "    plt.title('midi')\n",
    "    draw_midi_with_channel(plt.gca(), test_midi, colors=['Reds', 'Blues', 'Greens'], labels=[])\n",
    "\n",
    "plt.subplot(sub_figures, 1, pre_figures - 1)\n",
    "plt.title('note')\n",
    "plt.imshow(mask, aspect='auto', origin='lower', cmap='gray')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(sub_figures, 1, pre_figures)\n",
    "plt.title('onset')\n",
    "plt.imshow(onset, aspect='auto', origin='lower', cmap='gray')\n",
    "plt.colorbar()\n",
    "\n",
    "clustered_classes = []\n",
    "for i in range(N):\n",
    "    class_i = np.zeros(mask.shape)\n",
    "    class_i[positions[0], positions[1]] = (labels == i).astype(int)\n",
    "    clustered_classes.append(class_i)\n",
    "    plt.subplot(sub_figures, 1, i + pre_figures + 1)\n",
    "    plt.title(f'class{i}')\n",
    "    plt.imshow(class_i, aspect='auto', origin='lower', cmap='gray')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 音符层级的聚类\n",
    "先获取“音色无关转录”结果，并进行音符创建后处理，然后音符内用softmax平均得到特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必须先获取了转录结果\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.postprocess import output_to_notes_polyphonic\n",
    "\n",
    "mask = mask / mask.max()  # 归一化到0-1\n",
    "onset = onset / onset.max()  # 归一化到0-1\n",
    "\n",
    "note_events = output_to_notes_polyphonic(\n",
    "    mask, onset,\n",
    "    frame_thresh = 0.3,\n",
    "    onset_thresh = 0.4,\n",
    "    neighbor_trick = False,\n",
    "    midi_offset = 0 # 为了提取对应位置，不做偏移\n",
    ")\n",
    "\n",
    "embeddings = []\n",
    "for start, end, f, amp in note_events:\n",
    "    _mask = mask[f, start:end]  # (end-start, )\n",
    "    _emb = emb[:, f, start:end] # (18, end-start)\n",
    "    # weight = np.ones_like(_mask)\n",
    "    weight = _mask\n",
    "    # weight = _mask * _mask  # 发现用平方比用一次方好\n",
    "    # weight = np.sqrt(_mask)\n",
    "    # weight = np.exp(_mask * _mask)    # 用softmax，稍微强调一下最强的\n",
    "    weighted_emb = (_emb * weight).sum(axis=1)  # (18, )\n",
    "    normalized_emb = weighted_emb / np.linalg.norm(weighted_emb)\n",
    "    embeddings.append(normalized_emb)\n",
    "\n",
    "# N = 2\n",
    "N = 3 if test_midi is None else len(test_midi)  # 聚类数目\n",
    "\n",
    "similarity_matrix = cosine_similarity(embeddings)\n",
    "spectral = SpectralClustering(n_clusters=N, affinity='precomputed', assign_labels=\"cluster_qr\")\n",
    "labels = spectral.fit_predict(np.exp(similarity_matrix))\n",
    "\n",
    "# 画图 如果是两列，一张图大小用(6,3.5)合适，字的大小合适\n",
    "pre_figures = 2 + (0 if test_midi is None else 1)\n",
    "sub_figures = N + pre_figures\n",
    "\n",
    "plt.figure(figsize=(10, 5*sub_figures))\n",
    "\n",
    "if test_midi is not None:\n",
    "    plt.subplot(sub_figures, 1, 1)\n",
    "    plt.title('midi')\n",
    "    draw_midi_with_channel(plt.gca(), test_midi, colors=['Reds', 'Blues', 'Greens'], labels=[])\n",
    "\n",
    "plt.subplot(sub_figures, 1, pre_figures - 1)\n",
    "plt.title('note')\n",
    "plt.imshow(mask, aspect='auto', origin='lower', cmap='gray')\n",
    "# plt.colorbar()    # 缩放了不需要了\n",
    "\n",
    "plt.subplot(sub_figures, 1, pre_figures)\n",
    "plt.title('onset')\n",
    "plt.imshow(onset, aspect='auto', origin='lower', cmap='gray')\n",
    "# plt.colorbar()\n",
    "\n",
    "clustered_classes = []\n",
    "for i in range(N):\n",
    "    class_i = np.zeros(mask.shape)\n",
    "    indices = np.where(labels == i)[0]\n",
    "    for idx in indices:\n",
    "        start, end, f, amp = note_events[idx]\n",
    "        class_i[f, start+1:end] = amp\n",
    "        class_i[f, start] = 2 * amp # onset\n",
    "    clustered_classes.append(class_i)\n",
    "    plt.subplot(sub_figures, 1, i + pre_figures + 1)\n",
    "    plt.title(f'class{i}')\n",
    "    plt.imshow(class_i, aspect='auto', origin='lower', cmap='hot')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出为midi\n",
    "# 根据 labels 将 note_events 分为 N 类\n",
    "note_classes = [[] for _ in range(N)]\n",
    "for idx, label in enumerate(labels):\n",
    "    # 将音高加上24\n",
    "    start, end, pitch, amp = note_events[idx]\n",
    "    note_classes[label].append((start, end, pitch + 24, amp))\n",
    "\n",
    "from utils.midiarray import notes2midi, midi_merge\n",
    "midis = [notes2midi(note_class) for note_class in note_classes]\n",
    "midi_merge(midis).save(\"../test/clustered_output.mid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 取代聚类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_threshold = 0.2\n",
    "\n",
    "def iterate_filter(mask, emb, threshold=0.):\n",
    "    # 找到 mask 中最大值的位置\n",
    "    max_position = np.unravel_index(np.argmax(mask, axis=None), mask.shape)\n",
    "    # 提取对应的 emb 值\n",
    "    max_emb = emb[:, max_position[0], max_position[1]]\n",
    "\n",
    "    similarity_scores = np.tensordot(max_emb, emb*mask, axes=([0], [0])).reshape(emb.shape[1], emb.shape[2])\n",
    "\n",
    "    mask2 = mask - similarity_scores\n",
    "    # clip to avoid ramaining classes getting close\n",
    "    similarity_scores_clipped = np.where(similarity_scores > threshold, similarity_scores, 0)\n",
    "    emb2 = emb - similarity_scores_clipped * max_emb[:, None, None]\n",
    "    emb2 = emb2 / (np.linalg.norm(emb2, axis=0, keepdims=True) + 1e-6)\n",
    "    return similarity_scores, mask2, emb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_scores1, mask1, emb1 = iterate_filter(mask, emb, frame_threshold)\n",
    "\n",
    "plt.figure(figsize=(12, 15))\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.title('class_max')\n",
    "plt.imshow(similarity_scores1, aspect='auto', origin='lower', cmap='gray')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.imshow(similarity_scores1 > frame_threshold, aspect='auto', origin='lower', cmap='gray')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.imshow(mask1, aspect='auto', origin='lower', cmap='gray')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.title(\"after iteration 1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_scores2, mask2, emb2 = iterate_filter(mask1, emb1, frame_threshold)\n",
    "\n",
    "plt.figure(figsize=(12, 15))\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.title('class_max')\n",
    "plt.imshow(similarity_scores2, aspect='auto', origin='lower', cmap='gray')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.imshow(similarity_scores2 > frame_threshold, aspect='auto', origin='lower', cmap='gray')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.imshow(mask2, aspect='auto', origin='lower', cmap='gray')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.title(\"after iteration 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_scores3, mask3, emb3 = iterate_filter(mask2, emb2, frame_threshold)\n",
    "\n",
    "plt.figure(figsize=(12, 15))\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.title('class_max')\n",
    "plt.imshow(similarity_scores3, aspect='auto', origin='lower', cmap='gray')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.imshow(similarity_scores3 > frame_threshold, aspect='auto', origin='lower', cmap='gray')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.imshow(mask3, aspect='auto', origin='lower', cmap='gray')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.title(\"after iteration 3\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 2, figsize=(12, 10), sharex=True, sharey=True)\n",
    "\n",
    "axs[0, 0].set_title('1. AMT result')\n",
    "axs[0, 0].imshow(mask, aspect='auto', origin='lower')\n",
    "\n",
    "axs[1, 0].set_title('2. similarity to max')\n",
    "axs[1, 0].imshow(similarity_scores1, aspect='auto', origin='lower')\n",
    "\n",
    "axs[2, 0].set_title('3. note binarization')\n",
    "axs[2, 0].imshow(similarity_scores1 > frame_threshold, aspect='auto', origin='lower')\n",
    "\n",
    "axs[0, 1].set_title('4. probability correction')\n",
    "axs[0, 1].imshow(mask2, aspect='auto', origin='lower')\n",
    "\n",
    "axs[1, 1].set_title('5. similarity to max')\n",
    "axs[1, 1].imshow(similarity_scores2, aspect='auto', origin='lower')\n",
    "\n",
    "axs[2, 1].set_title('6. note binarization')\n",
    "axs[2, 1].imshow(similarity_scores2 > 0.3, aspect='auto', origin='lower')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导出为ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from septimbre import SepTimbreAMT_44100, Encoder_44100\n",
    "\n",
    "model_44100 = SepTimbreAMT_44100(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导出为ONNX\n",
    "model_44100.eval()\n",
    "\n",
    "input_audio = torch.randn((1, 1, 22050), dtype=torch.float32)    # (fixed, fixed, dynamic)\n",
    "\n",
    "with torch.no_grad():\n",
    "    torch.onnx.export(\n",
    "        model_44100,\n",
    "        (input_audio,),\n",
    "        'septimbre_44100.onnx',\n",
    "        input_names = [\"audio\"],\n",
    "        output_names = [\"onset\",\"frame\",\"embedding\"],\n",
    "        dynamic_shapes = {\n",
    "            'x': {2:'time'},    # same as model.forward input\n",
    "            # auto infer output shapes\n",
    "        },\n",
    "        dynamo=True,\n",
    "        verbose=False,\n",
    "        verify=True,\n",
    "        external_data=False,\n",
    "        autograd_inlining=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 先测试FLOPS RTFX 等指标\n",
    "Real-Time Factor(RTFX), FLOPS, actual inference time, memory peak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "uv pip install memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用onnxruntime推理\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "import torchaudio\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "from memory_profiler import memory_usage\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from utils.postprocess import cluster_notes\n",
    "from utils.midiarray import notes2midi\n",
    "\n",
    "input_wave_path = \"../data/inferMusic/flute_violin.wav\"  # 5min, 44100Hz\n",
    "N_clusters = 2\n",
    "ort_session = ort.InferenceSession(\"septimbre_44100.onnx\") # 创建一个推理session\n",
    "\n",
    "# 加载音频\n",
    "waveform, sample_rate = torchaudio.load(input_wave_path, normalize=True)\n",
    "waveform = waveform.unsqueeze(0).cpu()\n",
    "if waveform.shape[1] > 1:\n",
    "    waveform = waveform.mean(1, keepdim=True)\n",
    "waveform = waveform.numpy().astype(np.float32)\n",
    "wave_duration = waveform.shape[2] / sample_rate\n",
    "\n",
    "# 监控内存\n",
    "inference_time = 0\n",
    "def run_onnx():\n",
    "    global inference_time\n",
    "    t0 = time.time()\n",
    "    outputs = ort_session.run(None, {'audio': waveform})\n",
    "    t1 = time.time()\n",
    "    print(f\"inference time inside function: {t1 - t0:.3f} seconds\")\n",
    "    onset = outputs[0][0]   # (84, time)\n",
    "    frame = outputs[1][0]    # (84, time)\n",
    "    emb = outputs[2][0]     # (12, 84, time)\n",
    "    t2 = time.time()\n",
    "    note_events = cluster_notes(\n",
    "        frame, onset, emb, N_clusters, 24,\n",
    "        frame_thresh=0.32,\n",
    "        onset_thresh=0.4,\n",
    "        neighbor_trick=False\n",
    "    )\n",
    "    t3 = time.time()\n",
    "    print(f\"post-processing time inside function: {t3 - t2:.3f} seconds\")\n",
    "    inference_time = (t1 - t0) + (t3 - t2)\n",
    "    return note_events\n",
    "\n",
    "mem_usage, outputs = memory_usage((run_onnx, ), retval=True, interval=0.1)\n",
    "\n",
    "print(f\"inference time: {inference_time:.3f} seconds for {wave_duration:.3f} seconds audio with sampleRate {sample_rate} Hz, real-time factor: {inference_time/wave_duration:.4f}\")\n",
    "\n",
    "notes2midi(outputs).save(\"../test/septimbre.mid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 专门导出encoder部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_44100 = Encoder_44100(model).eval()\n",
    "\n",
    "input_audio = torch.randn((1, 1, 22050), dtype=torch.float32)    # (fixed, fixed, dynamic)\n",
    "\n",
    "with torch.no_grad():\n",
    "    torch.onnx.export(\n",
    "        encoder_44100,\n",
    "        (input_audio,),\n",
    "        'septimbre_encoder_44100.onnx',\n",
    "        input_names = [\"audio\"],\n",
    "        output_names = [\"embedding\"],\n",
    "        dynamic_shapes = {\n",
    "            'x': {2:'time'},    # same as model.forward input\n",
    "            # auto infer output shapes\n",
    "        },\n",
    "        dynamo=True,\n",
    "        verbose=False,\n",
    "        verify=True,\n",
    "        external_data=False,\n",
    "        autograd_inlining=False\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "timbreamt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
