# Septimbre: 音色分离转录

- sepamt_model.pth: 可以直接使用的模型
- sepamt_note_branch.pth: 预训练的音色无关转录分支
- septimbre_44100.onnx: sepamt_model.pth导出的输入为44100Hz的ONNX
- septimbre_encoder_44100.onnx: sepamt_model.pth中encoder分支、适配44100Hz的ONNX

## 音乐转录和音色编码是否相辅相成？

- DicephNet: 共享一部分编码分支——发现效果变差了（情理之中，因为参数量少了）
- rescale: 用音色无关转录的结果对音色编码的中间feature进行强制的幅度缩放——效果变差了。这本是留给attention的“强制注意力”，虽然attention没用，但是我认为这也可以强调某些地方，让音色编码的时候只关注重要的时频单元。不过竟然起了副作用。

因此这两个任务变为了独立的分支，仅仅共用CQT频谱，不再有交集。

## attention是否有用？

使用了FlowAttention、SlotAttention、BandAttention等（都在`~/model/attention.py`中），但效果均不显著。

想用attention的动机为：
- 卷积层仅仅关注局部，但音色分离需要关注全局，因此用attention收集全局信息。
- 从Hebb学习中可以发现，attention其实是可以加强聚类信息的。

在Hebb学习的理论下，attention不需要残差连接与FFN，但是结果表明这很容易导致类别融合，反而不好分离。通过tSNE的结果可以看出编码连在了一起——这是topk注意力带来的；但是不同类别之间也连起来了。这表明，attention缺失能加强聚类特征（连起来），但对于不能完全分离的地方，这样的“连接”会链式反应，直接将两个类别连起来。

而经典的Transformer是使用了残差连接和FFN的，但是实验表明加和不加没什么区别。。。

为了实现attention，我烧掉了不少脑细胞。本任务attention最大的问题在于：时频单元很多很多，但有大量是无效的（仅有音符所在时频单元对应的编码是有用的）。我先使用了O(N)的LinearAttention和FlowAttention，然后受Slot的启发，想出了“先用几个slots收集全局信息（作为Query），再用这几个slots反过来表示全局（作为KV）”的方法。为了减轻类别融合，我使用了tokp；为了让topk能聚焦到真正有用的音色编码，我用音色无关转录结果对音色编码进行了强制幅度缩放。我觉得虽然这里没用，但是肯定在某个地方大有作为！

## layernorm VS instanceNorm

layernorm专注于某个token，instancenorm使用的是全局，因此我认为instanceNorm会更好。直观上理解，InstanceNorm减去的是特征向量的均值，也就是让特征向量围绕原点——这是非常适合聚类的。而layernorm会破坏方向信息，比如\[1,1\]经过ln后就变为了\[0,0\]。但是实验表明其实没什么太大的差别？仅仅在四分离中LN稍显疲软。

## 损失函数

查考的文献的深度聚类损失都是用了相似度矩阵的MSE，也就是要求不同编码之间正交，这显然不利于聚类。当然也有其优点：可以简便计算。改进方法很多：比如对不同类的负相似度用小权值，比如将相似度从\[-1,1\]先仿射到\[0,1\]，再使用MSE或者BCE。后者是往相反方向优化的，可以用正反例加权弱化“反向”的要求。但是问题是这样做损失太大，和AMT损失不在一个数量级。所以换用了InfoNCE，不仅损失量级和AMT损失一致，还没有显式指明优化方向。最可贵的是，用InfoNCE就可以将单音色样本也纳入学习了——此时损失为0。因为单样本时，模型可能强行根据细微差别将同一类的音色编码拆分（理想情况为均匀分布再原点周围），而此时若要求方向一致可能会有反作用。

事实证明该损失确实有效，分离指标更加优秀。

## 编码维度取多少？

测试了12和16，发现16几乎没有提升，甚至有不如。所以选择了12。更少的维度没有实验。