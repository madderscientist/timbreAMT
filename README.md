# 基于人工智能的多音色自动复调音乐转录研究
目标是实现 轻量级的 音源分离 扒谱AI，其中音源分离不依赖训练集而是动态学习，即对训练集之外的音色有普适性。
- 轻量级：为了让研究切切实实落地应用。最终比baseline轻量了一半，成功部署到[notedigger](https://madderscientist.github.io/noteDigger/)中。其实如今已经有很多很优秀的分离扒谱商品了，而转换为token、用大模型的方法实现也有一统所有领域的趋势。所以选择轻量也是避其锋芒的做法，旨在探索一种全新的原理。
- 音源分离：根据音色进行分离，称为“盲源分离”更确切，因为目标是不依赖训练集。
- 扒谱：学名“音乐转录”，结合“音源分离”指“扒带”，即输入为多音色音频，输出多轨音符，每一轨对应一种音色。

现有音色分离转录方法的缺点是没有泛化性，体现在两点上：
1. 严重依赖数据集，后果是不认识训练集之外的音色。相比于“分离”他们更像是“分类”，需要制定好要有哪些类别的音色，然后通过喂数据使模型“记住”各个音色。特点是精度很高，特别是针对某一种乐器的扒谱模型。但是适用性太窄了，除非到MT3这种大模型时才会有显著突破。
2. 分离能力受结构限制。语音分离领域的一个难点是“说话人数目未知”，而如果做“分类”，则一定要确定类别数。所以二分离的模型不能用于三分离，适用性受限。

本研究的目的就是解决如上两点。人类对音色的跟踪完全不是这样的，一个婴儿就能区分出音色，而无需认识任何乐器，也就是说做的不是“音色的分类”而是“音色的区分”。当我们听到某音色的音符后，我们先会和记忆中的音色比较，如果相近就归为一类，如果不同则认为是新的音色；而“记忆”里的音色，一部分是之前习得（类比训练集），更重要的是听这个音频前面部分时建立起来的记忆，相当于动态学习音色、动态归类。这个“动态性”就是实现研究目标的关键。

由于直接实现最终目标过于困难，所以本研究将其拆解为两部分：
1. 先实现“音色无关转录”，即部根据音色分类，参考的是BasicPitch，并进行了改进。
2. 再实现“音色分离转录”，基本思想是对音色进行聚类，给“音色无关转录”的音符分配标签。

本研究的创新点：
1. 低成本、高精度数据集构建方法，客服了传统数据集采集、标注缺陷
2. 轻量级音色无关转录模型（参数量、运行开销减半，性能与基线持平）
3. 提出了联想记忆机制，实现了普适性的音色分离转录
4. 针对聚类的缺陷进行了多种后处理优化

一个失败了但我认为很有潜力的做法是，对每个时频单元编码为一个音色向量，其幅度表示音符在此的概率，方向表示音色。类似Hinton的Capsule。我认为模型规模足够大时可以实现。

## 文件与流程
9-11月阅读论文。尝试了几个模型，主要是MT3和BasicPitch。做了些小实验：
看到人声分离的论文大多使用mask，但相位直接用了原输入的，这合理吗？于是交换了两个音频的相位，发现相位才是表意的，无论是语音还是音乐。因此在“音色分离转录”阶段将相位也纳入编码了。

### 第一步：制作数据集
12月开始编写随机合成数据集的制作脚本，详见[data](data/README.md)文件夹。这一过程中写了一些通用工具代码存放于utils文件夹。此外还完成了torch下的CQT层等其他层。

### 第二步：音色无关转录
历时两个月（2025.01~2025.03初），完成了对标BasicPitch的模型设计与训练，见[basicAMT](basicamt/README.md)。主要贡献有：修复了很多bug、完善了随机合成数据集的制作、更改CQT的hop从384变为256、完成了模型的帧级评估、完成了模型输出到具体音符的转化函数。可以说，AMT模型开发的整个流程已经搭建完成。结果是参数量只有BasicPitch的一半，但效果相近。

其实在此之前尝试过直接完成音色分离转录，但是失败了。还完成了毕设的开题报告、文献翻译、开题答辩。

### 第三步：音色分离转录
历时一个月（2025.03初~2025.04初），在中期检查前结束了毕设的主要工作。

从Hopfield到Hebb到Attention，从LinearAttention到FlowAttention，在“音色无关转录”的基础上增加了聚类分支，实现了初步的音色分离转录，相关结果见[septimbre](septimbre/README.md)。得益于音色无关转录任务，找对了方向后进展非常快。

## 文件夹结构
```
├─basicamt “音色无关转录”
├─basicpitch 作为“音色无关转录”的baseline，对比用
|
├─septimbre “音色分离转录”
|
├─evaluate 模型评估
|
├─data 数据相关，如训练集、可视化
├─model 存放一些公用的torch.nn.Module
└─utils 存放一些公用的工具函数
```

## 碎碎念
这其实是我的毕业设计，自主选题。大学四年甚至高中的种种共同造就了这个课题。

初中时开始接触乐器和二次元，高一开始接触半音阶口琴（为了演奏二次元），但谱不够了，只能自己扒。靠耳朵显然不适合我这种新手，于是了解到wavetone，并被其扒谱方式深深折服——根据频域信息扒谱真是太方便了！于是做了个app：[哼歌扒谱](https://www.bilibili.com/video/BV18E411A7kC/)。不过当时不会对频域取对数，基频提取也用的是插件。基频提取显然远远不够，要实现wavetone的效果，我了解到了FFT，但是当时完全看不懂。

然后就高考了，扒谱是我择校的一半原因：冲着“信号”就认定了“电子信息”专业。另一半原因是觉得做硬件很酷，很想成为[Dimsmary](https://space.bilibili.com/5657589)这样的人，然后私信发现他是电子信息专业。高考是江苏前一千，最后被综评锁死到了SEU，身边人都很惋惜但其实我还挺满意：毕竟是电子信息强校。不出意外的，我接触到了很多硬件和信号处理知识，在大三的《数字信号处理》中终于学到了FFT，于是有了[noteDigger](https://madderscientist.github.io/noteDigger/)，算是达成一个目标。

不过我想学的知识、想掌握的技术已经在前5个学期中学完了。信息方向的下一步是通信，然而我对通信是一点不感兴趣。这才发现自己的目标虽然坚定但还是短浅。在大三下学期，以学院第四拿到了保研资格，难道要学通信了吗？或者选择信号处理方向？但我不想处理雷达信号，处理音频才合我意……这不就是AI干的事吗？于是决定跨保计算机。之前选修过AI方向的专业课，再补了点408相关知识，就开始陶瓷。

保研陶瓷本就费神，跨保更是鄙视链底端，不过最终还是以“智能科学与技术”专业保研了，脱离通信！导师是校外的，未来研究方向也不是通信，那毕设就没必要跟着老师走——做点自己喜欢的吧！于是就把这个课题推上去了。从24年9月就开始阅读论文，11月中旬正式确定为毕设选题，4月18日凌晨完成毕设论文（疑似全校第一），6月6日答辩。文献越读越简单，上手越做越难，前期屡屡碰壁，但从未后悔。虽然成果并不完美，但我还是挺骄傲的。

摘一段论文致谢吧：

    行文至此，总算了结了四年来的执念。说来好笑，这个课题的起因特别简单而纯粹——只是想演奏找不到谱子的动漫歌曲。没想到，对“谱”的执念一路指引着我走过了本科四年：为了学习信号处理，选择了东南大学电子信息；为了便于看谱，上线了一个谱库的微信小程序；为了制谱，写了一系列乐谱处理工具；为了便于扒谱，完成了一个平台；为了对图片简谱转调，初涉深度学习还小赚一笔；再到如今，竟已经能从学术角度研究扒谱并做出创新。虽然本项目的效果与我想要的还是相差甚远，但我仍然无比欣慰。

为什么谱对我这么重要？当初是特别喜欢某个音乐，喜欢到想永远掌握住，于是认为“能演奏便是拥有”。如今再想，大概是对音乐的尊重、对演奏的热爱吧。在这里我要特别提及两个组织：第一是[justice_eternal吧](https://tieba.baidu.com/f?kw=justice_eternal&ie=utf-8)，简称je吧，当时在这里获取了大量ACG曲谱，然后我开始用代码处理曲谱与音乐，是“梦开始的地方”。第二是[风之声口琴社](https://space.bilibili.com/354838640)，是我大学心灵的归宿，在琴房度过的每一天都与音乐相伴，还有一群可爱的人。