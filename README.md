# 基于人工智能的多音色自动复调音乐转录研究
目标是实现乐器无关的音源分离扒谱，其中音源分离完全依赖音色。

## 文件与流程
9-11月阅读论文。尝试了几个模型，主要是MT3和BasicPitch。做了些小实验：
看到人声分离的论文大多使用mask，但相位直接用了原输入的，这合理吗？于是交换了两个音频的相位，发现相位才是表意的（对于语音）。

### 第一步：制作数据集
12月开始编写随机合成数据集的制作脚本，详见[data](data/README.md)文件夹。这一过程中写了一些通用工具代码存放于utils文件夹。此外还完成了torch下的CQT层等其他层。

### 第二步：音色无关转录
历时两个月（2025.01~2025.03初），完成了对标BasicPitch的模型设计与训练，见[basicAMT](basicamt/README.md)。主要贡献有：修复了很多bug、完善了随机合成数据集的制作、更改CQT的hop从384变为256、完成了模型的帧级评估、完成了模型输出到具体音符的转化函数。可以说，AMT模型开发的整个流程已经搭建完成。

其实在此之前尝试过直接完成音色分离转录，但是失败了。还完成了毕设的开题报告、文献翻译、开题答辩。

### 第三步：音色分离转录
todo